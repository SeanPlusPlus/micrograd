# Glossary

## Activation Function
Function that introduces non-linearity to neural networks. Examples: ReLU, tanh, sigmoid.

## Backward Pass
Algorithm that computes gradients by walking the computational graph in reverse order.

## Bias
Parameter that allows a neuron to activate even when all inputs are zero. Like a "default mood."

## Computational Graph
Network of connected Value objects showing how operations depend on each other.

## Forward Pass
Computing outputs from inputs by following operations left to right.

## Gradient
Number showing how much the output changes if you change an input by 1 unit.

## Neuron
Basic building block that takes inputs, applies weights and bias, then an activation function.

## ReLU (Rectified Linear Unit)
Activation function: if input ≥ 0 → output = input, if input < 0 → output = 0.

## Value Object
"Smart number" that tracks both its value and how to compute gradients automatically.

## Weights
Parameters that control how much each input influences a neuron's output.
